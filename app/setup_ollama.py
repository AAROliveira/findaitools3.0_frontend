"""
Script para configurar Ollama automaticamente
Instala modelos recomendados e testa a configura√ß√£o
"""

import os
import sys
import subprocess
import time
from pathlib import Path

# Adiciona o diret√≥rio do app ao path
sys.path.append(str(Path(__file__).parent))

try:
    from ollama_config import (
        OllamaManager,
        setup_ollama_models,
        print_model_recommendations,
        test_ollama_setup,
    )
except ImportError:
    print(
        "‚ùå Erro ao importar ollama_config. Certifique-se que est√° no diret√≥rio correto."
    )
    sys.exit(1)


def check_ollama_installation():
    """Verifica se o Ollama est√° instalado"""
    try:
        result = subprocess.run(
            ["ollama", "--version"], capture_output=True, text=True, check=True
        )
        print(f"‚úÖ Ollama instalado: {result.stdout.strip()}")
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("‚ùå Ollama n√£o encontrado")
        return False


def install_ollama_instructions():
    """Mostra instru√ß√µes para instalar o Ollama"""
    print("\nüì• INSTRU√á√ïES PARA INSTALAR OLLAMA")
    print("=" * 40)
    print("1. Windows:")
    print("   - Baixe de: https://ollama.ai/download")
    print("   - Execute o instalador")
    print("   - Reinicie o terminal")
    print()
    print("2. Linux/Mac:")
    print("   curl -fsSL https://ollama.ai/install.sh | sh")
    print()
    print("3. Ap√≥s instala√ß√£o, execute:")
    print("   ollama serve")


def start_ollama_service():
    """Tenta iniciar o servi√ßo Ollama"""
    print("üöÄ Tentando iniciar servi√ßo Ollama...")

    try:
        # Tenta iniciar em background
        if os.name == "nt":  # Windows
            subprocess.Popen(
                ["ollama", "serve"], creationflags=subprocess.CREATE_NEW_CONSOLE
            )
        else:  # Linux/Mac
            subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
            )

        print("‚è≥ Aguardando servi√ßo inicializar...")
        time.sleep(5)

        # Verifica se est√° rodando
        manager = OllamaManager()
        if manager.is_running():
            print("‚úÖ Servi√ßo Ollama iniciado com sucesso!")
            return True
        else:
            print("‚ùå Falha ao iniciar servi√ßo automaticamente")
            return False

    except Exception as e:
        print(f"‚ùå Erro ao iniciar servi√ßo: {e}")
        return False


def setup_recommended_models():
    """Configura modelos recomendados"""
    print("\nü§ñ CONFIGURANDO MODELOS RECOMENDADOS")
    print("=" * 45)

    manager = OllamaManager()

    if not manager.is_running():
        print("‚ùå Ollama n√£o est√° rodando")
        if not start_ollama_service():
            print("Execute manualmente: ollama serve")
            return False

    print("üìã Modelos que ser√£o instalados:")
    print("  - nomic-embed-text (embedding, ~274MB)")
    print("  - deepseek-r1:1.5b (chat, ~1GB)")
    print()

    response = input("Continuar com a instala√ß√£o? (s/N): ").lower()
    if response not in ["s", "sim", "y", "yes"]:
        print("‚èπÔ∏è Instala√ß√£o cancelada")
        return False

    success, messages = setup_ollama_models(manager)

    print("\nüìä RESULTADO DA CONFIGURA√á√ÉO:")
    for message in messages:
        print(f"  {message}")

    if success:
        print("\n‚úÖ Configura√ß√£o conclu√≠da com sucesso!")
        return True
    else:
        print("\n‚ùå Algumas configura√ß√µes falharam")
        return False


def test_full_setup():
    """Testa configura√ß√£o completa"""
    print("\nüß™ TESTANDO CONFIGURA√á√ÉO COMPLETA")
    print("=" * 40)

    # Teste b√°sico
    if not test_ollama_setup():
        return False

    # Teste de embedding
    print("\nüîç Testando gera√ß√£o de embedding...")
    manager = OllamaManager()

    test_text = "Este √© um teste de embedding"
    embedding = manager.generate_embedding(test_text, "nomic-embed-text")

    if embedding:
        print(f"‚úÖ Embedding gerado: {len(embedding)} dimens√µes")
    else:
        print("‚ùå Falha na gera√ß√£o de embedding")
        return False

    # Teste de chat
    print("\nüí¨ Testando gera√ß√£o de resposta...")
    test_prompt = "Ol√°! Responda brevemente: o que √© intelig√™ncia artificial?"
    response = manager.generate_response(test_prompt, "deepseek-r1:1.5b")

    if response:
        print(f"‚úÖ Resposta gerada: {response[:100]}...")
    else:
        print("‚ùå Falha na gera√ß√£o de resposta")
        return False

    print("\nüéâ Todos os testes passaram!")
    return True


def create_ollama_env():
    """Cria arquivo .env com configura√ß√µes do Ollama"""
    env_file = Path(".env")

    ollama_config = """
# Configura√ß√µes do Ollama
USE_OLLAMA=true
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_CHAT_MODEL=deepseek-r1:1.5b

# Desabilita OpenAI quando usando Ollama
USE_LOCAL_LLM=true
# OPENAI_API_KEY=
"""

    if env_file.exists():
        content = env_file.read_text()
        if "USE_OLLAMA" not in content:
            # Adiciona configura√ß√µes do Ollama ao .env existente
            with open(env_file, "a") as f:
                f.write(ollama_config)
            print("‚úÖ Configura√ß√µes Ollama adicionadas ao .env")
    else:
        # Cria novo .env
        env_file.write_text(ollama_config.strip())
        print("‚úÖ Arquivo .env criado com configura√ß√µes Ollama")


def main():
    """Fun√ß√£o principal"""
    print("ü§ñ CONFIGURADOR AUTOM√ÅTICO DO OLLAMA")
    print("FindAI Tools RAG Chatbot")
    print("=" * 50)

    # Passo 1: Verificar instala√ß√£o
    if not check_ollama_installation():
        install_ollama_instructions()
        return False

    # Passo 2: Mostrar modelos recomendados
    print_model_recommendations()

    # Passo 3: Configurar modelos
    if not setup_recommended_models():
        return False

    # Passo 4: Testar configura√ß√£o
    if not test_full_setup():
        return False

    # Passo 5: Criar configura√ß√µes
    create_ollama_env()

    # Resultado final
    print("\n" + "=" * 50)
    print("üéâ CONFIGURA√á√ÉO OLLAMA CONCLU√çDA!")
    print("=" * 50)
    print("‚úÖ Modelos instalados e testados")
    print("‚úÖ Configura√ß√µes criadas")
    print()
    print("üìã Pr√≥ximos passos:")
    print("1. Execute o pipeline: python ../ingest/pipeline_complete.py")
    print("2. Inicie o servidor: python start_server.py")
    print("3. Teste a API: python test_api.py")
    print()
    print("üí° O sistema agora usa Ollama localmente!")

    return True


if __name__ == "__main__":
    success = main()
    if not success:
        print("\n‚ùå Configura√ß√£o n√£o foi conclu√≠da")
        print("Verifique os logs acima e tente novamente")
        sys.exit(1)
