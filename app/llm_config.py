"""
Configura√ß√µes de LLM para o RAG Chatbot
Suporte para OpenAI e modelos locais (llama-cpp-python)
"""
import os
import logging
from typing import Optional


logger = logging.getLogger(__name__)

# =========================
# Configura√ß√£o Ollama local
# =========================
import requests

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
EMBED_MODEL = os.getenv("OLLAMA_EMBED_MODEL", "nomic-embed-text")
LLM_MODEL = os.getenv("OLLAMA_LLM_MODEL", "gemma3n:e2b")

def get_embedding(text):
    """Gera embedding usando Ollama local (nomic-embed-text por padr√£o)"""
    response = requests.post(
        f"{OLLAMA_HOST}/api/embeddings",
        json={
            "model": EMBED_MODEL,
            "prompt": text
        }
    )
    return response.json()["embedding"]

def generate_response(context, question):
    """Gera resposta usando LLM local via Ollama (deepseek-r1:1.5b por padr√£o)"""
    prompt = f"{context}\n\nUsu√°rio: {question}\nAssistente:"
    response = requests.post(
        f"{OLLAMA_HOST}/api/generate",
        json={
            "model": LLM_MODEL,
            "prompt": prompt,
            "stream": False
        }
    )
    return response.json()["response"]



# Fun√ß√£o de configura√ß√£o para pipeline RAG: sempre prioriza Ollama local
def get_llm_config():
    """Retorna configura√ß√£o para uso local via Ollama (embeddings e LLM)"""
    config = {
        "type": "ollama-local",
        "embedding_fn": get_embedding,
        "llm_fn": generate_response,
        "embed_model": EMBED_MODEL,
        "llm_model": LLM_MODEL,
        "name": f"Ollama local: {LLM_MODEL} + {EMBED_MODEL}"
    }
    logger.info(f"‚úÖ LLM configurado: {config['name']}")
    return config


# As fun√ß√µes de OpenAI e fallback podem ser mantidas para compatibilidade, mas n√£o s√£o usadas no pipeline local


# N√£o √© mais necess√°rio, pois get_llm_config j√° retorna a config local


def get_fallback_config():
    """Configura√ß√£o de fallback (sem LLM espec√≠fico)"""
    logger.warning("‚ö†Ô∏è Usando configura√ß√£o de fallback - apenas recupera√ß√£o de contexto")

    return {"type": "fallback", "llm": None, "name": "Fallback (somente recupera√ß√£o)"}


def download_local_model():
    """
    Helper para baixar um modelo local recomendado
    Execute este m√©todo separadamente se quiser usar LLM local
    """
    import urllib.request

    model_url = "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.q4_0.gguf"
    model_path = "./models/llama-2-7b-chat.gguf"

    os.makedirs("./models", exist_ok=True)

    if os.path.exists(model_path):
        logger.info(f"‚úÖ Modelo j√° existe: {model_path}")
        return model_path

    logger.info(f"üì• Baixando modelo: {model_url}")
    logger.info("‚è≥ Isso pode levar v√°rios minutos...")

    try:
        urllib.request.urlretrieve(model_url, model_path)
        logger.info(f"‚úÖ Modelo baixado: {model_path}")
        return model_path
    except Exception as e:
        logger.error(f"‚ùå Erro ao baixar modelo: {e}")
        return None


# Exemplo de uso das fun√ß√µes Ollama
if __name__ == "__main__":
    # Teste das fun√ß√µes
    texto = "FindAI Tools √© uma plataforma de IA para produtividade."
    emb = get_embedding(texto)
    print(f"Embedding gerado (primeiros 5 valores): {emb[:5]}")

    contexto = "Texto recuperado mais relevante dos seus posts."
    pergunta = "Quais s√£o as principais fun√ß√µes do FakeFind?"
    resposta = generate_response(contexto, pergunta)
    print(f"Resposta gerada: {resposta}")
