"""
ConfiguraÃ§Ãµes e utilitÃ¡rios para Ollama
Suporte completo para embeddings e LLMs locais via Ollama
"""

import os
import json
import logging
import requests
from typing import Dict, List, Optional, Tuple
from pathlib import Path

logger = logging.getLogger(__name__)

# ConfiguraÃ§Ãµes padrÃ£o do Ollama
OLLAMA_BASE_URL = "http://127.0.0.1:11434"
OLLAMA_API_TIMEOUT = 60

# Modelos recomendados por categoria
RECOMMENDED_EMBEDDING_MODELS = {
    "nomic-embed-text": {
        "size_mb": 274,
        "description": "Excelente para contextos longos (2048 tokens), supera ada-002",
        "use_case": "Recomendado para RAG geral",
        "context_length": 2048,
    },
    "mxbai-embed-large": {
        "size_mb": 335,
        "description": "SOTA BERT-large, robusto para domÃ­nio amplo",
        "use_case": "Melhor qualidade geral",
        "context_length": 512,
    },
    "all-minilm": {
        "size_mb": 67,
        "description": "Extremamente leve e rÃ¡pido",
        "use_case": "Ideal para recursos limitados",
        "context_length": 256,
    },
    "snowflake-arctic-embed": {
        "size_mb": 568,
        "description": "MultilÃ­ngue, desempenho escalÃ¡vel",
        "use_case": "Textos multilÃ­ngues",
        "context_length": 512,
    },
}

RECOMMENDED_CHAT_MODELS = {
    "gemma3n:e2b": {
        "description": "Modelo eficiente, boa qualidade, recomendado para PT-BR",
        "use_case": "Recomendado para 32GB RAM, respostas em portuguÃªs"
    }
}


class OllamaManager:
    """Gerenciador para operaÃ§Ãµes com Ollama"""

    def __init__(self, base_url: str = OLLAMA_BASE_URL):
        self.base_url = base_url
        self.session = requests.Session()
        # Configure timeout nas requests, nÃ£o na session
        self.timeout = OLLAMA_API_TIMEOUT

    def is_running(self) -> bool:
        """Verifica se o Ollama estÃ¡ rodando"""
        try:
            response = self.session.get(
                f"{self.base_url}/api/tags", timeout=self.timeout
            )
            return response.status_code == 200
        except requests.exceptions.RequestException:
            return False

    def list_models(self) -> List[Dict]:
        """Lista modelos instalados"""
        try:
            response = self.session.get(
                f"{self.base_url}/api/tags", timeout=self.timeout
            )
            if response.status_code == 200:
                data = response.json()
                return data.get("models", [])
            return []
        except Exception as e:
            logger.error(f"Erro ao listar modelos: {e}")
            return []

    def is_model_installed(self, model_name: str) -> bool:
        """Verifica se um modelo estÃ¡ instalado"""
        models = self.list_models()
        installed_names = [model["name"] for model in models]
        return model_name in installed_names

    def pull_model(self, model_name: str) -> bool:
        """Baixa um modelo do Ollama"""
        try:
            logger.info(f"ğŸ“¥ Baixando modelo: {model_name}")

            payload = {"name": model_name}
            response = self.session.post(
                f"{self.base_url}/api/pull",
                json=payload,
                stream=True,
                timeout=self.timeout,
            )

            if response.status_code == 200:
                # Processa stream de download
                for line in response.iter_lines():
                    if line:
                        try:
                            data = json.loads(line)
                            if "status" in data:
                                print(f"ğŸ“¦ {data['status']}")
                            if data.get("error"):
                                logger.error(f"Erro no download: {data['error']}")
                                return False
                        except json.JSONDecodeError:
                            continue

                logger.info(f"âœ… Modelo {model_name} baixado com sucesso!")
                return True
            else:
                logger.error(f"Erro ao baixar modelo: {response.status_code}")
                return False

        except Exception as e:
            logger.error(f"Erro no download do modelo {model_name}: {e}")
            return False

    def generate_embedding(self, text: str, model: str) -> Optional[List[float]]:
        """Gera embedding usando Ollama"""
        try:
            payload = {"model": model, "prompt": text}

            response = self.session.post(
                f"{self.base_url}/api/embeddings", json=payload, timeout=self.timeout
            )

            if response.status_code == 200:
                data = response.json()
                return data.get("embedding")
            else:
                logger.error(f"Erro ao gerar embedding: {response.status_code}")
                return None

        except Exception as e:
            logger.error(f"Erro na geraÃ§Ã£o de embedding: {e}")
            return None

    def generate_response(self, prompt: str, model: str, **kwargs) -> Optional[str]:
        """Gera resposta usando modelo de chat do Ollama"""
        try:
            payload = {"model": model, "prompt": prompt, "stream": False, **kwargs}

            response = self.session.post(
                f"{self.base_url}/api/generate", json=payload, timeout=self.timeout
            )

            if response.status_code == 200:
                data = response.json()
                return data.get("response")
            else:
                logger.error(f"Erro ao gerar resposta: {response.status_code}")
                return None

        except Exception as e:
            logger.error(f"Erro na geraÃ§Ã£o de resposta: {e}")
            return None


def get_ollama_config() -> Dict:
    """Retorna configuraÃ§Ã£o recomendada do Ollama"""

    # ConfiguraÃ§Ã£o padrÃ£o
    embedding_model = os.getenv("OLLAMA_EMBEDDING_MODEL", "nomic-embed-text")
    chat_model = os.getenv("OLLAMA_CHAT_MODEL", "deepseek-r1:1.5b")

    config = {
        "embedding_model": embedding_model,
        "chat_model": chat_model,
        "base_url": os.getenv("OLLAMA_BASE_URL", OLLAMA_BASE_URL),
        "embedding_info": RECOMMENDED_EMBEDDING_MODELS.get(embedding_model, {}),
        "chat_info": RECOMMENDED_CHAT_MODELS.get(chat_model, {}),
    }

    return config


def setup_ollama_models(
    manager: Optional[OllamaManager] = None,
) -> Tuple[bool, List[str]]:
    """Configura modelos recomendados do Ollama"""

    if not manager:
        manager = OllamaManager()

    if not manager.is_running():
        logger.error("âŒ Ollama nÃ£o estÃ¡ rodando. Execute: ollama serve")
        return False, ["Ollama nÃ£o estÃ¡ rodando"]

    config = get_ollama_config()
    embedding_model = config["embedding_model"]
    chat_model = config["chat_model"]

    messages = []
    success = True

    # Verifica/baixa modelo de embedding
    if not manager.is_model_installed(embedding_model):
        logger.info(f"ğŸ“¥ Baixando modelo de embedding: {embedding_model}")
        if manager.pull_model(embedding_model):
            messages.append(f"âœ… Embedding model {embedding_model} instalado")
        else:
            messages.append(f"âŒ Falha ao instalar {embedding_model}")
            success = False
    else:
        messages.append(f"âœ… Embedding model {embedding_model} jÃ¡ instalado")

    # Verifica/baixa modelo de chat
    if not manager.is_model_installed(chat_model):
        logger.info(f"ğŸ“¥ Baixando modelo de chat: {chat_model}")
        if manager.pull_model(chat_model):
            messages.append(f"âœ… Chat model {chat_model} instalado")
        else:
            messages.append(f"âŒ Falha ao instalar {chat_model}")
            success = False
    else:
        messages.append(f"âœ… Chat model {chat_model} jÃ¡ instalado")

    return success, messages


def print_model_recommendations():
    """Imprime recomendaÃ§Ãµes de modelos"""
    print("ğŸ¤– MODELOS OLLAMA RECOMENDADOS")
    print("=" * 50)

    print("\nğŸ“Š MODELOS DE EMBEDDING:")
    for model, info in RECOMMENDED_EMBEDDING_MODELS.items():
        print(f"  {model}")
        print(f"    ğŸ’¾ Tamanho: {info['size_mb']} MB")
        print(f"    ğŸ“ DescriÃ§Ã£o: {info['description']}")
        print(f"    ğŸ¯ Uso: {info['use_case']}")
        print(f"    ğŸ“ Contexto: {info['context_length']} tokens")
        print()

    print("ğŸ’¬ MODELOS DE CHAT:")
    for model, info in RECOMMENDED_CHAT_MODELS.items():
        print(f"  {model}")
        print(f"    ğŸ’¾ Tamanho: {info['size_gb']} GB")
        print(f"    ğŸ“ DescriÃ§Ã£o: {info['description']}")
        print(f"    ğŸ¯ Uso: {info['use_case']}")
        print()


def test_ollama_setup():
    """Testa configuraÃ§Ã£o do Ollama"""
    print("ğŸ§ª TESTANDO CONFIGURAÃ‡ÃƒO OLLAMA")
    print("=" * 40)

    manager = OllamaManager()

    # Teste 1: Ollama rodando
    if manager.is_running():
        print("âœ… Ollama estÃ¡ rodando")
    else:
        print("âŒ Ollama nÃ£o estÃ¡ rodando")
        print("Execute: ollama serve")
        return False

    # Teste 2: Lista modelos
    models = manager.list_models()
    print(f"ğŸ“¦ Modelos instalados: {len(models)}")
    for model in models:
        print(f"  - {model['name']}")

    # Teste 3: ConfiguraÃ§Ã£o atual
    config = get_ollama_config()
    print(f"\nğŸ”§ ConfiguraÃ§Ã£o atual:")
    print(f"  Embedding: {config['embedding_model']}")
    print(f"  Chat: {config['chat_model']}")

    # Teste 4: Modelos necessÃ¡rios
    embedding_installed = manager.is_model_installed(config["embedding_model"])
    chat_installed = manager.is_model_installed(config["chat_model"])

    print(f"\nğŸ“‹ Status dos modelos:")
    print(
        f"  Embedding ({config['embedding_model']}): {'âœ…' if embedding_installed else 'âŒ'}"
    )
    print(f"  Chat ({config['chat_model']}): {'âœ…' if chat_installed else 'âŒ'}")

    if not embedding_installed or not chat_installed:
        print("\nğŸ’¡ Para instalar modelos faltando:")
        if not embedding_installed:
            print(f"  ollama pull {config['embedding_model']}")
        if not chat_installed:
            print(f"  ollama pull {config['chat_model']}")

    return embedding_installed and chat_installed


if __name__ == "__main__":
    # Executar testes quando chamado diretamente
    print_model_recommendations()
    print("\n" + "=" * 50 + "\n")
    test_ollama_setup()
